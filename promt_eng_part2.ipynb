{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/exercises/ex5/ex5_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kekuLuANuoMP"
   },
   "source": [
    "### LLM Prompting and Prompt Engineering Part 2\n",
    "\n",
    "In part 2, we experiment with prompting instruction-tuned Large Language Models (LLMs), and evaluate their performance on a linguistic annotation task involving structured outputs.\n",
    "\n",
    "The goal of this assignment is to gain some experience working with instruction-tuned LLMs. To this end, you will learn how to\n",
    "\n",
    "- query an instruction-tuned LLM with default chat templates (`Llama-3.2-3B-Instruct`)\n",
    "- parse LLM outputs for structured responses using JSON and `Pydantic`\n",
    "- implement error handling for edge cases where the model fails to output the expected data format.\n",
    "\n",
    "The task we use for this purpose is a simple Tokenization and Part-of-Speech tagging task using data taken from Universal Dependencies.\n",
    "\n",
    "To facilitate working with LLMs, we will again use the Unsloth library. Note that Unsloth provides both freeware and closed-source proprietary software. For our purposes, the freeware is sufficient! For more information on Unsloth, see the docs here.\n",
    "\n",
    "This notebook is adapted from [this example](https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing) by Unsloth.\n",
    "\n",
    "\n",
    "### NOTE: Expected execution times\n",
    "We have provided expected execution times throughout the notebook as a guide. These are intended to be approximate, but should give you some idea for what to expect. If your runtimes far exceed these expected execution times, you may want to consider modifying your approach. These are denoted with ‚åõ .\n",
    "\n",
    "### NOTE: GPU Usage\n",
    "It is expected that you load the model onto a GPU for inference. For other parts of the code, such as data preparation, a GPU is not necessary. To avoid waiting for resources unnecessarily, we recommend doing as much as you can on a CPU instance and change the runtime type as necessary. We've highlight the cells that need a GPU with ‚ö°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue7JmxZoXtV2"
   },
   "source": [
    "## 1) Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 62389,
     "status": "ok",
     "timestamp": 1732924004452,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install levenshtein\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1732924004868,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "c_LEUjzo4sfI",
    "outputId": "5bf32c64-0aad-4bf2-a827-d44ccc34a720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Found Unsloth version 2024.11.10 but expected 2024.10.2.\n",
      "3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "Fri Nov 29 23:46:44 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   55C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Your runtime has 13.6 gigabytes of available RAM\n",
      "\n",
      "Not using a high-RAM runtime\n"
     ]
    }
   ],
   "source": [
    "# check unsloth version\n",
    "expected_version = '2024.10.2'\n",
    "unsloth_version = !pip list | grep -P 'unsloth\\s+' | grep -Po '\\S+$'\n",
    "if unsloth_version[0] != expected_version:\n",
    "    print(f\"Warning! Found Unsloth version {unsloth_version[0]} but expected {expected_version}.\")\n",
    "\n",
    "# check python version\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# check gpu info\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# check RAM info\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VDpOQwNwxi1"
   },
   "source": [
    "## 2) Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 772,
     "referenced_widgets": [
      "3f8e51a07eaf4d23807a9d6143393cef",
      "62fe48926afd41fcab67be0abc89255a",
      "ddb5df32b52a44f28b8656ae2a2b868f",
      "80dbf3005c944b33aa1419947352b40f",
      "5be3d6b958444f309d59f958b36cf4a4",
      "2767fc3e4c3642a8899bc4100cfc412e",
      "37c40303947a45b99106462f31b92950",
      "c61d03c3a1614112b5cb3276e6589ff7",
      "cd23c49e56cc4bde96e25c65386980b4",
      "9226f346be5b47eaa66b75937c6fdf9b",
      "921bcd4a092b443fb1b44f4486375305",
      "591a0997b469465a979995fabe746185",
      "c2e338ea232d4b849c75d004667378c9",
      "48cf391db62b4faea16859172a3fad4d",
      "86b1282eccde4807bbe1a4a573dbd4a7",
      "68519ae719d2462aac63e7efa5603ca3",
      "9af61e0ac8fa4f2db270083750f7d5ce",
      "9072cf806a114c1691911c410bb4f872",
      "ee6bb028079e4506b62729ba973646f6",
      "16bff4c4cb3d415db865fb6a5d0d5680",
      "f913bc568d18440e8b1d775d7971f1e4",
      "4847e81436cd4dffaa8784eccd175f93",
      "7b8f9a12c01d496eb9acf469668befdb",
      "875717ec1bf34a97bb9d395f47300156",
      "a5ac9feae139431a9c476fd542f90cd2",
      "62a88b510fb0476ca89074001cc561f1",
      "61730c748ba5453dada80bbb6fa80de7",
      "f785cd582a7f41a9a2825c8569a94dbc",
      "3515095848504c9c98a3f170551b97ea",
      "9ebd00711fa548798a250f6476aaf2d3",
      "55a4f261331c4cf394275047bd256d16",
      "c3e14239db544fa399d5a616cf16cc1e",
      "36ebb6213eb64efeb33c87de12145242",
      "7d5f611665384fafaa784ef39b5dc183",
      "ff245d13dee1473187b74490a8c9525d",
      "9136ca766cfe4b84b62d8a6820026d80",
      "780d40fab242435681d39226f14e2c70",
      "5b3be287de9f42b4a85a6a3cadd7d9df",
      "1e5db832717a44c19c175234b5b74e20",
      "fc191f1866444b1b8397b15c304049be",
      "598544d8875541d2aabb504dea23d585",
      "ec9124a35d204544a53615475b24e846",
      "95fde0185a3648b4a6435d7231ffcbce",
      "ef0fe3c72edc40d39e386d752cc97730",
      "68da65709e9f4648ac6e564f2e73b803",
      "2b0efab908c34d8fb3f44b2d9c4fc551",
      "91c03b4ffebe43e4a43a9d6ba00d5b24",
      "fca46b682089495bb1577061daf5cfef",
      "53c5cc032d9c4e1793c0facdde5459de",
      "f7ee884154fb409a90a2a276cceb13b3",
      "d220db0c7c494a939ada36e4b3b73eff",
      "8200ffb2faf748bf896ee7044b1b03cc",
      "db0c26ab3ff545d2a37b9cbc4907185b",
      "27d5753ffb8f4658a95f1e42302144d1",
      "4c37516c2bea438986f90a7240c493e2"
     ]
    },
    "executionInfo": {
     "elapsed": 93342,
     "status": "ok",
     "timestamp": 1732924098207,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "86ddbc8f-ca09-4135-8ebd-90a8bf3216bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.2.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8e51a07eaf4d23807a9d6143393cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a0997b469465a979995fabe746185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8f9a12c01d496eb9acf469668befdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5f611665384fafaa784ef39b5dc183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da65709e9f4648ac6e564f2e73b803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# Note, here we specify the instruction-tuned version of Llama-3.2-3B\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7owhdpPE7i8U"
   },
   "source": [
    "## 3) Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1732924098963,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "IcrSdhyr7ecF",
    "outputId": "42f4fceb-b9c6-49f4-da56-21e01779c215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1408\n",
      "Test data: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>upos</th>\n",
       "      <th>xpos</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>\"...there is no companion quite so devoted, so...</td>\n",
       "      <td>\" ... there is no companion quite so devoted ,...</td>\n",
       "      <td>PUNCT PUNCT PRON VERB DET NOUN ADV ADV ADJ PUN...</td>\n",
       "      <td>`` , EX VBZ DT NN RB RB JJ , RB JJ , RB JJ CC ...</td>\n",
       "      <td>[{'token': '\"', 'pos': 'PUNCT'}, {'token': '.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>Great computer repair store, highly recommended.</td>\n",
       "      <td>Great computer repair store , highly recommend...</td>\n",
       "      <td>ADJ NOUN NOUN NOUN PUNCT ADV VERB PUNCT</td>\n",
       "      <td>JJ NN NN NN , RB VBN .</td>\n",
       "      <td>[{'token': 'Great', 'pos': 'ADJ'}, {'token': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>You wear your heart on your sleeve ... and sin...</td>\n",
       "      <td>You wear your heart on your sleeve ... and sin...</td>\n",
       "      <td>PRON VERB PRON NOUN ADP PRON NOUN PUNCT CCONJ ...</td>\n",
       "      <td>PRP VBP PRP$ NN IN PRP$ NN , CC IN PRP VBP DT ...</td>\n",
       "      <td>[{'token': 'You', 'pos': 'PRON'}, {'token': 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>for Books that Speak for Themselves....</td>\n",
       "      <td>for Books that Speak for Themselves ....</td>\n",
       "      <td>ADP NOUN PRON VERB ADP PRON PUNCT</td>\n",
       "      <td>IN NNS WDT VBP IN PRP ,</td>\n",
       "      <td>[{'token': 'for', 'pos': 'ADP'}, {'token': 'Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>yuck !!</td>\n",
       "      <td>yuck !!</td>\n",
       "      <td>INTJ PUNCT</td>\n",
       "      <td>UH .</td>\n",
       "      <td>[{'token': 'yuck', 'pos': 'INTJ'}, {'token': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "578   \"...there is no companion quite so devoted, so...   \n",
       "1146   Great computer repair store, highly recommended.   \n",
       "382   You wear your heart on your sleeve ... and sin...   \n",
       "583             for Books that Speak for Themselves....   \n",
       "966                                             yuck !!   \n",
       "\n",
       "                                                 tokens  \\\n",
       "578   \" ... there is no companion quite so devoted ,...   \n",
       "1146  Great computer repair store , highly recommend...   \n",
       "382   You wear your heart on your sleeve ... and sin...   \n",
       "583            for Books that Speak for Themselves ....   \n",
       "966                                             yuck !!   \n",
       "\n",
       "                                                   upos  \\\n",
       "578   PUNCT PUNCT PRON VERB DET NOUN ADV ADV ADJ PUN...   \n",
       "1146            ADJ NOUN NOUN NOUN PUNCT ADV VERB PUNCT   \n",
       "382   PRON VERB PRON NOUN ADP PRON NOUN PUNCT CCONJ ...   \n",
       "583                   ADP NOUN PRON VERB ADP PRON PUNCT   \n",
       "966                                          INTJ PUNCT   \n",
       "\n",
       "                                                   xpos  \\\n",
       "578   `` , EX VBZ DT NN RB RB JJ , RB JJ , RB JJ CC ...   \n",
       "1146                             JJ NN NN NN , RB VBN .   \n",
       "382   PRP VBP PRP$ NN IN PRP$ NN , CC IN PRP VBP DT ...   \n",
       "583                             IN NNS WDT VBP IN PRP ,   \n",
       "966                                                UH .   \n",
       "\n",
       "                                                 target  \n",
       "578   [{'token': '\"', 'pos': 'PUNCT'}, {'token': '.....  \n",
       "1146  [{'token': 'Great', 'pos': 'ADJ'}, {'token': '...  \n",
       "382   [{'token': 'You', 'pos': 'PRON'}, {'token': 'w...  \n",
       "583   [{'token': 'for', 'pos': 'ADP'}, {'token': 'Bo...  \n",
       "966   [{'token': 'yuck', 'pos': 'INTJ'}, {'token': '...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "dataset_url = \"https://raw.githubusercontent.com/tannonk/prompting_exercise/refs/heads/main/data/en_ewt-ud-dev-pos.json\"\n",
    "df = pd.read_json(dataset_url, lines=True)\n",
    "\n",
    "# For each input sentence, we'll build the target as a list of dictionaries containing keys for the token and pos tag. This is what we want our LLM annotator to predict.\n",
    "df['target'] = df.apply(lambda x: [{'token': token, 'pos': pos} for token, pos in zip(x['tokens'].split(), x['upos'].split())], axis=1)\n",
    "\n",
    "# We'll sample 100 items for testing purposes\n",
    "test_data = df.sample(n=100, random_state=seed)\n",
    "train_data = df.drop(test_data.index)\n",
    "\n",
    "print(f\"Train data: {len(train_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")\n",
    "\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc4OwDRnwIpf"
   },
   "source": [
    "### TODO: Inspect and describe the data\n",
    "\n",
    "üìù‚ùì What are the fields and their corresponding values in the dataframe?\n",
    "\n",
    "üìù‚ùì What is the difference between `upos` and `xpos`?\n",
    "\n",
    "üìù‚ùì What is the distribution of `upos` labels in the `test_data`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the test set: 1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'NOUN': 219,\n",
       "         'PUNCT': 146,\n",
       "         'VERB': 124,\n",
       "         'ADJ': 111,\n",
       "         'PRON': 106,\n",
       "         'ADP': 99,\n",
       "         'DET': 88,\n",
       "         'AUX': 72,\n",
       "         'ADV': 69,\n",
       "         'PROPN': 69,\n",
       "         'CCONJ': 41,\n",
       "         'PART': 17,\n",
       "         'NUM': 16,\n",
       "         'SCONJ': 13,\n",
       "         'INTJ': 6,\n",
       "         'SYM': 3,\n",
       "         'X': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pos = [tok[\"pos\"] for sent in test_data.target.values.tolist() for tok in sent]\n",
    "dist = Counter(pos)\n",
    "print(f\"Total number of tokens in the test set: {len(pos)}\")\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSqsJYxpwqD4"
   },
   "source": [
    "### TODO: Define the basic `PromptTemplate`\n",
    "\n",
    "Note, you can reuse the solution from part 1 of this exercise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1732925073586,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "bx9wBnZx7eZz"
   },
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    def __init__(self, task_description, system = None):\n",
    "        self.task_description = task_description\n",
    "        self.system = system\n",
    "\n",
    "    def zero_shot_prompt(self, input_sentence):\n",
    "\n",
    "        if self.system: # for system prompt \n",
    "          chat = [{'role': 'system', 'content': self.system},\n",
    "                  {'role': 'user', 'content': f'{self.task_description}\\n\\n{input_sentence}'}]\n",
    "        else:\n",
    "          chat = [{'role': 'user', 'content': f'{self.task_description}\\n\\n{input_sentence}'}]\n",
    "\n",
    "        return chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "## 3.2 ChatTemplates\n",
    "\n",
    "Instruction-tuned models are typically finetuned using a predefined `ChatTemplate`.\n",
    "This means that when using them for inference, it is important that we use the correct `ChatTemplate` in order to avoid \"confusing\" the model.\n",
    "\n",
    "You can find more information about model `ChatTemplates` for Huggingface models [here](https://huggingface.co/docs/transformers/en/chat_templating).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1732924164790,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "wvUc0oCp-oMa",
    "outputId": "be059544-6e54-4db8-83a1-0cb3d770f359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 July 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\n",
      "\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\n",
      "\n",
      "\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\n",
      "\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\n",
      "\n",
      "\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content'] %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\n",
      "\n",
      "\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# load the chat_template from unsloth (note, the logic is similar when using native Huggingface, but here we're using Unsloth!)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\", # for Llama-3.1 and Llama-3.2 models\n",
    ")\n",
    "\n",
    "# Inspect the template (note, it looks more complicated than it is!)\n",
    "print(tokenizer.chat_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PE4qKNHbZU-"
   },
   "source": [
    "### TODO: Prepare your inputs using the `ChatTemplate` for the model.\n",
    "\n",
    "Note, you should be able to drop your custom `PromptTemplate` string into the model's default `ChatTemplate`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1732924197816,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "mJYe8Vfx-oKE"
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "prompt_template = PromptTemplate(\"Please tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence 'I hate prompts!' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4. ! - PUNCT\\n\\nYour turn:\")\n",
    "for idx, row in test_data.iterrows():\n",
    "    input_text = row[\"sentence\"]\n",
    "    prompt = prompt_template.zero_shot_prompt(input_text)\n",
    "    prompts.append(tokenizer.apply_chat_template(prompt, tokenize = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xobpn1UhxZZo"
   },
   "source": [
    "## 4) Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeMEUBEebsFd"
   },
   "source": [
    "### TODO: Define a function to run inference efficiently with an LLM\n",
    "\n",
    "Note, you can use the same inference function from part 1 of this exercise here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1732924202132,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "kkICV3hAAUQ9"
   },
   "outputs": [],
   "source": [
    "# Set up our inference pipeline for generation\n",
    "\n",
    "# We'll set some default generation args that we'll pass to our inference function\n",
    "# Following best practices, we'll use Pydantic class which helps with validation.\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Generation_Args(BaseModel):\n",
    "    max_new_tokens: int\n",
    "    temperature: float\n",
    "    top_k: int\n",
    "    top_p: float\n",
    "    repetition_penalty: float\n",
    "    do_sample: bool\n",
    "    use_cache: bool\n",
    "    min_p: float\n",
    "    num_return_sequences: int\n",
    "\n",
    "# Here are some default generation args\n",
    "generation_args = Generation_Args(\n",
    "    max_new_tokens = 1024, # note, for this task, we're setting the max_new_tokens to be more appropriate\n",
    "    temperature = 1.0,\n",
    "    top_k = 0,\n",
    "    top_p = 1.0,\n",
    "    repetition_penalty = 1.0,\n",
    "    do_sample = True,\n",
    "    use_cache = True,\n",
    "    min_p = 0.1,\n",
    "    num_return_sequences = 1\n",
    ")\n",
    "\n",
    "\n",
    "def run_batched_inference(prompts, model, tokenizer, batch_size=10, generation_args=generation_args):\n",
    "    \"\"\"\n",
    "    Runs batched inference on a list of prompts using a given model and tokenizer.\n",
    "\n",
    "    Set the batch_size to control the number of prompts processed in each batch.\n",
    "    Depending on the length of your prompts and model size the batch size may need to be adjusted.\n",
    "\n",
    "    Args:\n",
    "        prompts (list[str]): List of prompts that are passed to the model\n",
    "        model (): The model used for generation\n",
    "        tokenizer (): The tokenizer used for encoding and decoding the prompts\n",
    "        batch_size (int): Number of prompts to run batched inference on.\n",
    "\n",
    "    Returns:\n",
    "        List[str] containing generated outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement the logic for efficient inference with LLM\n",
    "    outputs = []\n",
    "    for batch_idx in range(0, len(prompts), batch_size):\n",
    "      batch = prompts[batch_idx: batch_idx+batch_size]\n",
    "\n",
    "      tokenizer.padding_side='left' #pad to the left\n",
    "      tokenizer.pad_token = tokenizer.eos_token #pad the input with the eos token\n",
    "      inputs = tokenizer(batch, return_tensors = \"pt\", padding=True).to(model.device)\n",
    "      out = model.generate(**inputs,\n",
    "                           max_new_tokens = generation_args.max_new_tokens,\n",
    "                           temperature = generation_args.temperature,\n",
    "                           top_k = generation_args.top_k,\n",
    "                           top_p = generation_args.top_p,\n",
    "                           repetition_penalty = generation_args.repetition_penalty,\n",
    "                           do_sample = generation_args.do_sample,\n",
    "                           use_cache = generation_args.use_cache,\n",
    "                           min_p = generation_args.min_p,\n",
    "                           num_return_sequences = generation_args.num_return_sequences\n",
    "                           )\n",
    "\n",
    "\n",
    "      decoded = tokenizer.batch_decode(out)\n",
    "      decoded = [output.replace(tokenizer.pad_token, \"\") for output in decoded]\n",
    "\n",
    "      outputs.extend(decoded)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1732924203710,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "zW36ZKxobxPn",
    "outputId": "a08d484a-9ee1-49b4-b895-7d1aede2fdd5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nPlease tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence \\'I hate prompts!\\' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4. ! - PUNCT\\n\\nYour turn:\\n\\n\"...there is no companion quite so devoted, so communicative, so loving and so mesmerizing as a rat.\"<|eot_id|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BcvKAh58g0q"
   },
   "source": [
    "## 4.1) Run Inference\n",
    "\n",
    "### TODO: run inference!\n",
    "\n",
    "‚åõ 10-20 mins\n",
    "\n",
    "‚ö° GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 524990,
     "status": "ok",
     "timestamp": 1732924739132,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "5cHPJJyJ825R"
   },
   "outputs": [],
   "source": [
    "# TODO: inference\n",
    "pred = run_batched_inference(prompts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ILZF5EYISwi"
   },
   "source": [
    "## 5) Structured output validation\n",
    "\n",
    "LLMs output text. But in practice, we often want structured data that we can process further with other automatic processes.\n",
    "\n",
    "For this purpose, JSON is a good target data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixlnaY4Jbw8k"
   },
   "source": [
    "### TODO: Define a processing pipeline that extracts and validates the JSON response from the LLM.\n",
    "\n",
    "Hint: For this you should use a combination of [`Regex`](https://www.w3schools.com/python/python_regex.asp) and [`Pydantic`](https://docs.pydantic.dev/latest/).\n",
    "\n",
    "The output should be a valid json object with the following structure:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"token\": \"there\", \"pos\": \"DET\"}, # each dict contains a token and its corresponding POS-Tag.\n",
    "    {\"token\": \"is\", \"pos\": \"VERB\"},\n",
    "    {\"token\": \"no\", \"pos\": \"ADJ\"},\n",
    "    {\"token\": \"companion\", \"pos\": \"NOUN\"},\n",
    "    {\"token\": \"quite\", \"pos\": \"ADV\"},\n",
    "    {\"token\": \"so\", \"pos\": \"ADV\"},\n",
    "    {\"token\": \"devoted\", \"pos\": \"ADV\"},\n",
    "    {\"token\": \"so\", \"pos\": \"ADV\"},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1732924758081,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "-RjRwgHLVUR2"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1732924924444,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "8Skma6ohHlkN"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "class Tok(BaseModel):\n",
    "    token: str\n",
    "    pos: str\n",
    "\n",
    "    @field_validator(\"pos\")\n",
    "    def validate_pos(cls, tag: str) -> str: # validate the pos labels \n",
    "        valid_pos_tags = [\"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\",\n",
    "                          \"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\",\n",
    "                          \"PUNCT\", \"SYM\", \"X\"]\n",
    "        if tag in valid_pos_tags:\n",
    "            return tag\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pos tag!\")\n",
    "\n",
    "def parse_and_validate(output):\n",
    "    token_and_pos = re.findall(r\"\\n\\d{,2}\\.\\s?(.+) - (.+)\", output)\n",
    "    valid = []\n",
    "    for token, pos in token_and_pos:\n",
    "        try:\n",
    "            tok = Tok(token = token, pos = pos)\n",
    "            valid.append(tok)\n",
    "        except(ValueError):\n",
    "            continue\n",
    "    return [{\"token\": tok.token, \"pos\": tok.pos} for tok in valid[4:]]\n",
    "\n",
    "processed_outputs = list(map(lambda output: parse_and_validate(output), pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbijgXU4jaVx"
   },
   "source": [
    "## 6) *Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1732924764661,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "QkD2UbYZ7RDX"
   },
   "outputs": [],
   "source": [
    "# Below is some boilerplate evaluation code. You should not need to make any changes here.\n",
    "\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "def evaluate_instance(target: List[Dict], prediction: List[Dict]):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of tokenization and part-of-speech (POS) tagging between a target and a predicted sequence.\n",
    "\n",
    "    Args:\n",
    "        target (List[Dict]): A list of dictionaries representing the target tokens and POS tags.\n",
    "        prediction (List[Dict]): A list of dictionaries representing the predicted tokens and POS tags.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the token-level accuracy ('Token Acc') and POS accuracy ('POS Acc').\n",
    "    \"\"\"\n",
    "\n",
    "    # If there is no prediction, return zero accuracies\n",
    "    if prediction is None:\n",
    "        return {'Token Acc': 0, 'POS Acc': 0}\n",
    "\n",
    "    # Extract tokens and POS tags from the target and prediction lists\n",
    "    target_tokens = [item['token'] for item in target]\n",
    "    target_pos = [item['pos'] for item in target]\n",
    "    pred_tokens = [item['token'] for item in prediction]\n",
    "    pred_pos = [item['pos'] for item in prediction]\n",
    "\n",
    "    # Get alignment operations between the target and predicted tokens using Levenshtein.opcodes()\n",
    "    opcodes = Levenshtein.opcodes(target_tokens, pred_tokens)\n",
    "\n",
    "    # Initialize aligned lists to store tokens and POS tags after alignment\n",
    "    aligned_target_tokens = []\n",
    "    aligned_target_pos = []\n",
    "    aligned_pred_tokens = []\n",
    "    aligned_pred_pos = []\n",
    "\n",
    "    # Iterate over each operation in the alignment\n",
    "    for tag, i1, i2, j1, j2 in opcodes:\n",
    "        # \"equal\" means the tokens in this range are identical in both sequences\n",
    "        if tag == 'equal':\n",
    "            aligned_target_tokens.extend(target_tokens[i1:i2])\n",
    "            aligned_target_pos.extend(target_pos[i1:i2])\n",
    "            aligned_pred_tokens.extend(pred_tokens[j1:j2])\n",
    "            aligned_pred_pos.extend(pred_pos[j1:j2])\n",
    "        # \"replace\" means tokens in this range are different between the target and prediction\n",
    "        elif tag == 'replace':\n",
    "            aligned_target_tokens.extend(target_tokens[i1:i2])\n",
    "            aligned_target_pos.extend(target_pos[i1:i2])\n",
    "            aligned_pred_tokens.extend(pred_tokens[j1:j2])\n",
    "            aligned_pred_pos.extend(pred_pos[j1:j2])\n",
    "        # \"insert\" means tokens were added in the prediction that are not in the target\n",
    "        elif tag == 'insert':\n",
    "            aligned_target_tokens.extend(['<MISSING>'] * (j2 - j1))  # Add placeholders for missing target tokens\n",
    "            aligned_target_pos.extend(['<MISSING>'] * (j2 - j1))      # Add placeholders for missing target POS tags\n",
    "            aligned_pred_tokens.extend(pred_tokens[j1:j2])\n",
    "            aligned_pred_pos.extend(pred_pos[j1:j2])\n",
    "        # \"delete\" means tokens are present in the target but missing in the prediction\n",
    "        elif tag == 'delete':\n",
    "            aligned_target_tokens.extend(target_tokens[i1:i2])\n",
    "            aligned_target_pos.extend(target_pos[i1:i2])\n",
    "            aligned_pred_tokens.extend(['<MISSING>'] * (i2 - i1))    # Add placeholders for missing predicted tokens\n",
    "            aligned_pred_pos.extend(['<MISSING>'] * (i2 - i1))       # Add placeholders for missing predicted POS tags\n",
    "\n",
    "    # Calculate token-level accuracy\n",
    "    # We only consider positions where both target and prediction have valid tokens (i.e., not '<MISSING>')\n",
    "    correct_tokens = [\n",
    "        1 if tgt == pred else 0\n",
    "        for tgt, pred in zip(aligned_target_tokens, aligned_pred_tokens)\n",
    "        if tgt != '<MISSING>' and pred != '<MISSING>'\n",
    "    ]\n",
    "    token_accuracy = np.mean(correct_tokens) if correct_tokens else 0\n",
    "\n",
    "    # Calculate POS accuracy\n",
    "    # Only consider positions where tokens match and are not '<MISSING>'\n",
    "    correct_pos = [\n",
    "        1 if tgt_pos == pred_pos else 0\n",
    "        for tgt_tok, pred_tok, tgt_pos, pred_pos in zip(aligned_target_tokens, aligned_pred_tokens, aligned_target_pos, aligned_pred_pos)\n",
    "        if tgt_tok == pred_tok and tgt_tok != '<MISSING>'\n",
    "    ]\n",
    "    pos_accuracy = np.mean(correct_pos) if correct_pos else 0\n",
    "\n",
    "    return {'Token Acc': token_accuracy, 'POS Acc': pos_accuracy}\n",
    "\n",
    "def get_results(test_data: pd.DataFrame, processed_outputs: List[List[Dict]]):\n",
    "    \"\"\"\n",
    "    Returns a summary dataframe by taking the average of the all results for tokenization and pos-tagging.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(len(processed_outputs)):\n",
    "        results.append(evaluate_instance(test_data.iloc[i]['target'], processed_outputs[i]))\n",
    "\n",
    "    results = pd.DataFrame(results).mean()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1732924934930,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "g54ebTc-1VYB",
    "outputId": "f9c2a77d-b653-431a-d260-36f442942e87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token Acc</th>\n",
       "      <td>0.875727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS Acc</th>\n",
       "      <td>0.743870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "Token Acc    0.875727\n",
       "POS Acc      0.743870\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the results, you should be able to pass your test_data DataFrame and the processed_outputs from above...\n",
    "get_results(test_data, processed_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhjNb5QClvTS"
   },
   "source": [
    "## 7) Manipulating the system prompt\n",
    "\n",
    "The system prompt is part of the `ChatTemplate` that can help to steer the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ4NOohvb6Iu"
   },
   "source": [
    "### TODO: Customise the system prompt for the intended task and re-run inference\n",
    "\n",
    "Note, this is an experiment. You should try a few different system prompts and report the resulting performance in your report.\n",
    "\n",
    "\n",
    "üìù‚ùì What was the best system prompt you considered?\n",
    "\n",
    "üìù‚ùì Were you able to improve the performance by manipulating the system prompt? Please discuss.\n",
    "\n",
    "‚åõ 10-20 mins (per experiment run)\n",
    "\n",
    "‚ö° GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1732925084568,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "yQRBsLTIhoGI"
   },
   "outputs": [],
   "source": [
    "# use the task description as the system prompt\n",
    "prompts = []\n",
    "system = \"\"\"\n",
    "            Your task is to tokenize and Part-of-Speech tag the given sentence.\n",
    "\n",
    "            Please only use the following universal POS tags:\n",
    "\n",
    "            \"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\",\n",
    "            \"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\",\n",
    "            \"PUNCT\", \"SYM\", \"X\"\n",
    "\n",
    "            Please format your answer as:\n",
    "\n",
    "            1. token - tag\n",
    "            2. token - tag\n",
    "            ...\n",
    "\n",
    "            Please include all the punctuations as well.\n",
    "         \"\"\"\n",
    "prompt_template = PromptTemplate(\"Please tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence 'I hate prompts!' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4. ! - PUNCT\\n\\nYour turn:\",\n",
    "                                 system = system)\n",
    "for idx, row in test_data.iterrows():\n",
    "    input_text = row[\"sentence\"]\n",
    "    prompt = prompt_template.zero_shot_prompt(input_text)\n",
    "    prompts.append(tokenizer.apply_chat_template(prompt, tokenize = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1732925955420,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "tdPrq3ZzYW62",
    "outputId": "a46204f6-09bb-4981-ed30-40442dc70aeb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n            Your task is to tokenize and Part-of-Speech tag the given sentence.\\n\\n            Please only use the following universal POS tags: \\n            \\n            \"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\",\\n            \"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\",\\n            \"PUNCT\", \"SYM\", \"X\"\\n\\n            Please format your answer as:\\n\\n            1. token - tag\\n            2. token - tag\\n            ...\\n\\n            Please include all the punctuations as well.\\n         <|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nPlease tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence \\'I hate prompts!\\' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4. ! - PUNCT\\n\\nYour turn:\\n\\nGreat computer repair store, highly recommended.<|eot_id|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 390680,
     "status": "ok",
     "timestamp": 1732925530458,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "8o5ida_EYrLb"
   },
   "outputs": [],
   "source": [
    "pred = run_batched_inference(prompts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1732925927263,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "PvoPWR0VbM5L",
    "outputId": "a59f0378-453a-4c12-d9ef-2f51f2240760"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n            Your task is to tokenize and Part-of-Speech tag the given sentence.\\n\\n            Please only use the following universal POS tags: \\n            \\n            \"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\",\\n            \"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\",\\n            \"PUNCT\", \"SYM\", \"X\"\\n\\n            Please format your answer as:\\n\\n            1. token - tag\\n            2. token - tag\\n           ...\\n\\n            Please include all the punctuations as well.\\n         <|start_header_id|>user<|end_header_id|>\\n\\nPlease tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence \\'I hate prompts!\\' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4.! - PUNCT\\n\\nYour turn:\\n\\nGreat computer repair store, highly recommended.<|start_header_id|>assistant<|end_header_id|>\\n\\nHere is the tokenization and POS tagging of the sentence \"Great computer repair store, highly recommended\":\\n\\n1. Great - ADJ\\n2. computer - NOUN\\n3. repair - NOUN\\n4. store - NOUN\\n5., - PUNCT (comma)\\n6. highly - ADV\\n7. recommended - VERB\\n8.. - PUNCT (period)\\n\\nNote that the comma (, ) is a PUNCT tag, and the period (.) is also a PUNCT tag. The comma is used to separate the items in the list, and the period is used to end the sentence.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1732925902132,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "56LHkNWUbfLm",
    "outputId": "35d70548-2211-42b4-a433-59dda0f493cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token Acc</th>\n",
       "      <td>0.918594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS Acc</th>\n",
       "      <td>0.739250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "Token Acc    0.918594\n",
       "POS Acc      0.739250\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_and_validate(output):\n",
    "    token_and_pos = re.findall(r\"\\n\\d{,2}\\.\\s?(.+) - (.+)\", output)\n",
    "    valid = []\n",
    "    for token, pos in token_and_pos:\n",
    "        try:\n",
    "            tok = Tok(token = token, pos = pos)\n",
    "            valid.append(tok)\n",
    "        except(ValueError):\n",
    "            continue\n",
    "    return [{\"token\": tok.token, \"pos\": tok.pos} for tok in valid]\n",
    "\n",
    "processed_outputs = list(map(lambda output: parse_and_validate(output), pred))\n",
    "get_results(test_data, processed_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1732926272357,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "ue-3YaOycRE3"
   },
   "outputs": [],
   "source": [
    "# use the task description as the system prompt, plus an example\n",
    "prompts = []\n",
    "system = \"\"\"\n",
    "            Your task is to tokenize and Part-of-Speech tag the given sentence.\n",
    "\n",
    "            Please only use the following universal POS tags:\n",
    "\n",
    "            \"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\",\n",
    "            \"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\",\n",
    "            \"PUNCT\", \"SYM\", \"X\"\n",
    "\n",
    "            Please format your answer as:\n",
    "\n",
    "            1. token - tag\n",
    "            2. token - tag\n",
    "            ...\n",
    "\n",
    "            Please include all the punctuations as well, but do not include the type of punctuations in the tag.\n",
    "\n",
    "            To give an example:\n",
    "\n",
    "            Asimov likes robots. ->\n",
    "\n",
    "            1. Asimov - PROPN\n",
    "            2. likes - VERB\n",
    "            3. robots - NOUN\n",
    "            4. . - PUNCT\n",
    "\n",
    "            Good luck!\n",
    "         \"\"\"\n",
    "prompt_template = PromptTemplate(\"Please tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence 'I hate prompts!' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4. ! - PUNCT\\n\\nYour turn:\",\n",
    "                                 system = system)\n",
    "for idx, row in test_data.iterrows():\n",
    "    input_text = row[\"sentence\"]\n",
    "    prompt = prompt_template.zero_shot_prompt(input_text)\n",
    "    prompts.append(tokenizer.apply_chat_template(prompt, tokenize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 262313,
     "status": "ok",
     "timestamp": 1732926553470,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "EfYGge-5dK6f"
   },
   "outputs": [],
   "source": [
    "pred = run_batched_inference(prompts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1732926932117,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "MXIhrefmffdf",
    "outputId": "1fcc2a25-e1d6-4b81-d19a-97e61fdfab2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n\\n            Your task is to tokenize and Part-of-Speech tag the given sentence.\\n\\n            Please only use the following universal POS tags: \\n            \\n            \"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\",\\n            \"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\",\\n            \"PUNCT\", \"SYM\", \"X\"\\n\\n            Please format your answer as:\\n\\n            1. token - tag\\n            2. token - tag\\n           ...\\n\\n            Please include all the punctuations as well, but do not include the type of punctuations in the tag.\\n\\n            To give an example:\\n\\n            Asimov likes robots. ->\\n\\n            1. Asimov - PROPN\\n            2. likes - VERB\\n            3. robots - NOUN\\n            4.. - PUNCT\\n\\n            Good luck!\\n         <|start_header_id|>user<|end_header_id|>\\n\\nPlease tokenize and then Part-of-Speech tag the following sentence with the Universal POS tags.\\nTo give an example, the sentence \\'I hate prompts!\\' should be tokenized and tagged as:\\n1. I - PRON\\n2. hate - VERB\\n3. prompts - NOUN\\n4.! - PUNCT\\n\\nYour turn:\\n\\nGreat computer repair store, highly recommended.<|start_header_id|>assistant<|end_header_id|>\\n\\n1. Great - ADJ\\n2. computer - NOUN\\n3. repair - NOUN\\n4. store - NOUN\\n5., - PUNCT\\n6. highly - ADV\\n7. recommended - VERB\\n8.. - PUNCT<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1732926985180,
     "user": {
      "displayName": "Vincent Fan",
      "userId": "14619032297107357788"
     },
     "user_tz": -60
    },
    "id": "DihtO1H4f0Cb",
    "outputId": "679d9747-ed91-4518-8270-acc72c0bfb87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token Acc</th>\n",
       "      <td>0.921324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS Acc</th>\n",
       "      <td>0.766147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "Token Acc    0.921324\n",
       "POS Acc      0.766147\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_outputs = list(map(lambda output: parse_and_validate(output), pred))\n",
    "get_results(test_data, processed_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKiPb2fvv8d_"
   },
   "source": [
    "---\n",
    "\n",
    "## 8) Lab report\n",
    "\n",
    "üìù‚ùì Write your lab report here addressing all questions in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Report \n",
    "\n",
    "*(Answers to questions are marked with ‚ùì)*\n",
    "\n",
    "#### **1. Introduction**\n",
    "\n",
    "We experimented with prompting the instruction-tuned version of Llama-3.2-3B model on a tokenization and Part-of-Speech (PoS) tagging task. We also experimented with different system prompts. The best setting yielded a tokenization accuracy of 0.92 and a PoS accuracy of 0.77. \n",
    "\n",
    "#### **2. Dataset**\n",
    "\n",
    "(‚ùì inspect the data) Our dataset contains five fields: the sentence in string (`sentence`), the pre-tokenized version of the sentence (`tokens`) where tokens were separated by whitespace, their universal pos tags (`upos`), a more fine-grained version of their pos tags (`xpos`) and a `target` field containing the desired results in the format`{'token': TOKEN, 'pos': POS}` for each word in the sentence, where the universal pos tags are used. The universal pos tags (`upos`) cover the core part-of-speech categories such as `NOUN` and `VERB`, whereas `xpos` also provided a more detailed indication of the word form, for example `NNS` for plural nouns. Among the test data of 1200 tokens, `NOUN` labels occur most frequently (219 occurrences), followed by `PUNC` (punctuations, 146 occurrences) and `VERB` (124, occurrences). Some labels are relatively rare in the data, for example, `INTJ` (interjections) only appears 6 times.\n",
    "\n",
    "#### **3. Model and prompting**\n",
    "\n",
    "We prompted the instruction-tuned version of Llama-3.2-3B model under three different settings: 1) No system prompt, 2) Task description as system prompt and 3) Task description and an example as system prompt. \n",
    "\n",
    "#### **4. Results and Discussions**\n",
    "\n",
    "The results are summarized in `Table 1`.\n",
    "\n",
    "|System prompt| Token acc | POS acc|\n",
    "|:-|-:|-:|\n",
    "|No|0.88|0.74|\n",
    "|Task description|0.92|0.74|\n",
    "|Task description + example|0.92|0.77|\n",
    "\n",
    "**Table 1** Test acc under different settings \n",
    "\n",
    "(‚ùì best system prompt) Using both the task description and an example as the system prompt gave us the best token and POS accuracy (0.92 and 0.77 respectively). (‚ùì improvement) Using this system prompt improved the token acc by 4% and 3% for POS acc. This small boost might be explicated by the capability of the system prompts to guide the model. In our case, we helped the model to focus on the task at hand, and by giving an example, implicitly instructed (apart from the overt instructions in the task description) it to use the Universal POS tags, which might explain the improvement in POS acc. The example also helped to limit the format of the model output, making it easier for the regex function to capture the output, hence potentially reducing false negative rate. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16bff4c4cb3d415db865fb6a5d0d5680": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1e5db832717a44c19c175234b5b74e20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2767fc3e4c3642a8899bc4100cfc412e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27d5753ffb8f4658a95f1e42302144d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b0efab908c34d8fb3f44b2d9c4fc551": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7ee884154fb409a90a2a276cceb13b3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d220db0c7c494a939ada36e4b3b73eff",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "3515095848504c9c98a3f170551b97ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36ebb6213eb64efeb33c87de12145242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37c40303947a45b99106462f31b92950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f8e51a07eaf4d23807a9d6143393cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62fe48926afd41fcab67be0abc89255a",
       "IPY_MODEL_ddb5df32b52a44f28b8656ae2a2b868f",
       "IPY_MODEL_80dbf3005c944b33aa1419947352b40f"
      ],
      "layout": "IPY_MODEL_5be3d6b958444f309d59f958b36cf4a4"
     }
    },
    "4847e81436cd4dffaa8784eccd175f93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48cf391db62b4faea16859172a3fad4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee6bb028079e4506b62729ba973646f6",
      "max": 184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16bff4c4cb3d415db865fb6a5d0d5680",
      "value": 184
     }
    },
    "4c37516c2bea438986f90a7240c493e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53c5cc032d9c4e1793c0facdde5459de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55a4f261331c4cf394275047bd256d16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "591a0997b469465a979995fabe746185": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c2e338ea232d4b849c75d004667378c9",
       "IPY_MODEL_48cf391db62b4faea16859172a3fad4d",
       "IPY_MODEL_86b1282eccde4807bbe1a4a573dbd4a7"
      ],
      "layout": "IPY_MODEL_68519ae719d2462aac63e7efa5603ca3"
     }
    },
    "598544d8875541d2aabb504dea23d585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b3be287de9f42b4a85a6a3cadd7d9df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5be3d6b958444f309d59f958b36cf4a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61730c748ba5453dada80bbb6fa80de7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62a88b510fb0476ca89074001cc561f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3e14239db544fa399d5a616cf16cc1e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_36ebb6213eb64efeb33c87de12145242",
      "value": "‚Äá54.6k/54.6k‚Äá[00:00&lt;00:00,‚Äá870kB/s]"
     }
    },
    "62fe48926afd41fcab67be0abc89255a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2767fc3e4c3642a8899bc4100cfc412e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_37c40303947a45b99106462f31b92950",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "68519ae719d2462aac63e7efa5603ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68da65709e9f4648ac6e564f2e73b803": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2b0efab908c34d8fb3f44b2d9c4fc551",
       "IPY_MODEL_91c03b4ffebe43e4a43a9d6ba00d5b24",
       "IPY_MODEL_fca46b682089495bb1577061daf5cfef"
      ],
      "layout": "IPY_MODEL_53c5cc032d9c4e1793c0facdde5459de"
     }
    },
    "780d40fab242435681d39226f14e2c70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95fde0185a3648b4a6435d7231ffcbce",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ef0fe3c72edc40d39e386d752cc97730",
      "value": "‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá31.3MB/s]"
     }
    },
    "7b8f9a12c01d496eb9acf469668befdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_875717ec1bf34a97bb9d395f47300156",
       "IPY_MODEL_a5ac9feae139431a9c476fd542f90cd2",
       "IPY_MODEL_62a88b510fb0476ca89074001cc561f1"
      ],
      "layout": "IPY_MODEL_61730c748ba5453dada80bbb6fa80de7"
     }
    },
    "7d5f611665384fafaa784ef39b5dc183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff245d13dee1473187b74490a8c9525d",
       "IPY_MODEL_9136ca766cfe4b84b62d8a6820026d80",
       "IPY_MODEL_780d40fab242435681d39226f14e2c70"
      ],
      "layout": "IPY_MODEL_5b3be287de9f42b4a85a6a3cadd7d9df"
     }
    },
    "80dbf3005c944b33aa1419947352b40f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9226f346be5b47eaa66b75937c6fdf9b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_921bcd4a092b443fb1b44f4486375305",
      "value": "‚Äá2.24G/2.24G‚Äá[00:26&lt;00:00,‚Äá263MB/s]"
     }
    },
    "8200ffb2faf748bf896ee7044b1b03cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86b1282eccde4807bbe1a4a573dbd4a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f913bc568d18440e8b1d775d7971f1e4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4847e81436cd4dffaa8784eccd175f93",
      "value": "‚Äá184/184‚Äá[00:00&lt;00:00,‚Äá4.67kB/s]"
     }
    },
    "875717ec1bf34a97bb9d395f47300156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f785cd582a7f41a9a2825c8569a94dbc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3515095848504c9c98a3f170551b97ea",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "9072cf806a114c1691911c410bb4f872": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9136ca766cfe4b84b62d8a6820026d80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_598544d8875541d2aabb504dea23d585",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec9124a35d204544a53615475b24e846",
      "value": 9085657
     }
    },
    "91c03b4ffebe43e4a43a9d6ba00d5b24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8200ffb2faf748bf896ee7044b1b03cc",
      "max": 454,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db0c26ab3ff545d2a37b9cbc4907185b",
      "value": 454
     }
    },
    "921bcd4a092b443fb1b44f4486375305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9226f346be5b47eaa66b75937c6fdf9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95fde0185a3648b4a6435d7231ffcbce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9af61e0ac8fa4f2db270083750f7d5ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ebd00711fa548798a250f6476aaf2d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5ac9feae139431a9c476fd542f90cd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ebd00711fa548798a250f6476aaf2d3",
      "max": 54598,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55a4f261331c4cf394275047bd256d16",
      "value": 54598
     }
    },
    "c2e338ea232d4b849c75d004667378c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9af61e0ac8fa4f2db270083750f7d5ce",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9072cf806a114c1691911c410bb4f872",
      "value": "generation_config.json:‚Äá100%"
     }
    },
    "c3e14239db544fa399d5a616cf16cc1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c61d03c3a1614112b5cb3276e6589ff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd23c49e56cc4bde96e25c65386980b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d220db0c7c494a939ada36e4b3b73eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db0c26ab3ff545d2a37b9cbc4907185b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ddb5df32b52a44f28b8656ae2a2b868f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c61d03c3a1614112b5cb3276e6589ff7",
      "max": 2242762780,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd23c49e56cc4bde96e25c65386980b4",
      "value": 2242762567
     }
    },
    "ec9124a35d204544a53615475b24e846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ee6bb028079e4506b62729ba973646f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef0fe3c72edc40d39e386d752cc97730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f785cd582a7f41a9a2825c8569a94dbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7ee884154fb409a90a2a276cceb13b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f913bc568d18440e8b1d775d7971f1e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc191f1866444b1b8397b15c304049be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fca46b682089495bb1577061daf5cfef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27d5753ffb8f4658a95f1e42302144d1",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4c37516c2bea438986f90a7240c493e2",
      "value": "‚Äá454/454‚Äá[00:00&lt;00:00,‚Äá12.8kB/s]"
     }
    },
    "ff245d13dee1473187b74490a8c9525d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e5db832717a44c19c175234b5b74e20",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fc191f1866444b1b8397b15c304049be",
      "value": "tokenizer.json:‚Äá100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
